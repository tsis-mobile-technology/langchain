{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92fd5242",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CallbackManager\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "from os.path import expanduser\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e7a952",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expanduser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\7B\\\\ggml-model-q4_0.gguf\")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\13B-Chat\\\\ggml-model-q4_k_m.gguf\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\M-SOLAR-10.7B-V1\\\\ggml-model-q4_k_m.gguf\")\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\M-SOLAR-10.7B-V2\\\\ggml-model-q4_k_m.gguf\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\Code-13B-Instruct\\\\ggml-model-q4_k_m.gguf\")\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\Code-34B-Instruct\\\\ggml-model-q4_k_m.gguf\")\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model_path \u001b[38;5;241m=\u001b[39m expanduser(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mProgramming\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mllama.cpp\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCode-34B-Python\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mggml-model-q4_k_m.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m n_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m      9\u001b[0m max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'expanduser' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\7B\\\\ggml-model-q4_0.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\13B-Chat\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\M-SOLAR-10.7B-V1\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\M-SOLAR-10.7B-V2\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\Code-13B-Instruct\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\Code-34B-Instruct\\\\ggml-model-q4_k_m.gguf\")\n",
    "model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\Code-34B-Python\\\\ggml-model-q4_k_m.gguf\")\n",
    "n_batch = 128\n",
    "max_new_tokens = 512\n",
    "temperature = 0.7\n",
    "top_p = 0.95\n",
    "top_k = 40\n",
    "repetition_penalty = 1.1\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\M-WIZARD-13B\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\C-WIZARD-33B-Python\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\16K-Vicuna-13B\\\\ggml-model-q4_k_m.gguf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6736de",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager: CallbackManager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "n_gpu_layers = 40\n",
    "# Change this value based on your model and your GPU VRAM pool.\n",
    "  \n",
    "llm = LlamaCpp(\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "\tn_batch=n_batch,\n",
    "    model_path=model_path,\n",
    "    temp=temperature,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    ctx_size=4096,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    repeat_penalty=repetition_penalty,\n",
    "    callback_manager=callback_manager, \n",
    "    verbose=True, # Verbose is required to pass to the callback manager\n",
    "    streaming=False\n",
    ")\n",
    "model = Llama2Chat(llm=llm)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chain = LLMChain(llm=model, prompt=prompt_template, memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\n",
    "    chain.run(\"\"\"### System Prompt\n",
    "              Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
    "        ### User Message\n",
    "              In Python, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month\n",
    "        ### Assistant\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    chain.run(text=\"\"\"### System Prompt\n",
    "        ### User Message\n",
    "              Tell me more about other code.\n",
    "        ### Assistant\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(\n",
    "#     chain.run(\n",
    "#         text=\"\"\"[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
    "#         In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month\n",
    "#         [/INST]\"\"\"\n",
    "#     )\n",
    "# )\n",
    "# print(chain.run(text=\"[INST]\\nTell me more about other code.[\\INST]\\n\\n\"))\n",
    "\n",
    "# print(\n",
    "#     chain.run(\n",
    "#         text=\"[INST]\\nWhat can I see in Vienna? Propose a few locations. Names only, no details.[\\INST]\\n\\n\"\n",
    "#     )\n",
    "# )\n",
    "# print(chain.run(text=\"[INST]\\nTell me more about #2.[\\INST]\\n\\n\"))\n",
    "\n",
    "# print(\n",
    "#     chain.run(\n",
    "#         text=\"대한민국에 있는 유명한 산 이름을 10개 정도 나열해주세요.\"\n",
    "#     )\n",
    "# )\n",
    "# print(chain.run(text=\"Tell me more about #2.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
