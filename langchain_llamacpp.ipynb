{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92fd5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "from os.path import expanduser\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e39b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e7a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\7B\\\\ggml-model-q4_0.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\13B-Chat\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\M-SOLAR-10.7B-V1\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\M-SOLAR-10.7B-V2\\\\ggml-model-q4_k_m.gguf\")\n",
    "model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\Code-13B-Instruct\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\Code-34B-Instruct\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\Code-34B-Python\\\\ggml-model-q4_k_m.gguf\")\n",
    "n_batch = 128\n",
    "max_new_tokens = 512\n",
    "temperature = 0.7\n",
    "top_p = 0.95\n",
    "top_k = 40\n",
    "repetition_penalty = 1.1\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\M-WIZARD-13B\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\C-WIZARD-33B-Python\\\\ggml-model-q4_k_m.gguf\")\n",
    "# model_path = expanduser(\"D:\\\\Programming\\\\llama.cpp\\\\models\\\\16K-Vicuna-13B\\\\ggml-model-q4_k_m.gguf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6736de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\28006030\\.conda\\envs\\langchain1\\Lib\\site-packages\\langchain_core\\utils\\utils.py:159: UserWarning: WARNING! temp is not default parameter.\n",
      "                temp was transferred to model_kwargs.\n",
      "                Please confirm that temp is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\28006030\\.conda\\envs\\langchain1\\Lib\\site-packages\\langchain_core\\utils\\utils.py:159: UserWarning: WARNING! max_new_tokens is not default parameter.\n",
      "                max_new_tokens was transferred to model_kwargs.\n",
      "                Please confirm that max_new_tokens is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\28006030\\.conda\\envs\\langchain1\\Lib\\site-packages\\langchain_core\\utils\\utils.py:159: UserWarning: WARNING! ctx_size is not default parameter.\n",
      "                ctx_size was transferred to model_kwargs.\n",
      "                Please confirm that ctx_size is what you intended.\n",
      "  warnings.warn(\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "callback_manager: CallbackManager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "n_gpu_layers = 40\n",
    "# Change this value based on your model and your GPU VRAM pool.\n",
    "  \n",
    "llm = LlamaCpp(\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "\tn_batch=n_batch,\n",
    "    model_path=model_path,\n",
    "    temp=temperature,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    ctx_size=4096,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    repeat_penalty=repetition_penalty,\n",
    "    callback_manager=callback_manager, \n",
    "    verbose=True, # Verbose is required to pass to the callback manager\n",
    "    streaming=False\n",
    ")\n",
    "model = Llama2Chat(llm=llm)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chain = LLMChain(llm=model, prompt=prompt_template, memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc20a01e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "chain.run(\"[INST]Hi. Python 으로 웹 개발을 하려고 합니다. 어떤 Python UI Framework가 좋은지 추천하고 간단한 설명, 샘플 프로그램을 보여주세요.[/INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16baea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [/INST]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(text=\"[INST]\\nTell me more about other code.[\\INST]\\n\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b662acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\28006030\\.conda\\envs\\langchain1\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <<SYS>>\n",
      "             [<<SYS>> ] >>>\n",
      "\n",
      "        ### System Prompt\n",
      "             [<<SYS>> ] >>> Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "    ```python\n",
      "    # Python code\n",
      "\n",
      "    def main():\n",
      "\n",
      "\n",
      "\n",
      "        return\n",
      "\n",
      "\n",
      "    if __name__ == '__main__':\n",
      "\n",
      "            main()\n",
      "\n",
      "\n",
      "    ```\n",
      "\n",
      "```python\n",
      "# Python code\n",
      "import os\n",
      "\n",
      "for filename in os.listdir(os.getcwd())):\n",
      "if  if os.path.isfile((filename)))):\n",
      "        print('The file I found is named ' + filename)')')')')')')')')')))))))'\n",
      "\n",
      "    ```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "# Python code\n",
      "import os\n",
      "\n",
      "for filename in os.listdir(os.getcwd())):\n",
      "if  if os.path.isfile(filename)))):\n",
      "        print('The file I found is named ' + filename)')')')')')')')')))))))'\n",
      "\n",
      "    ```\n",
      "\n",
      "```\n",
      "[/INST\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\n",
    "    chain.run(\"\"\"### System Prompt\n",
    "              Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
    "        ### User Message\n",
    "              In Python, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month\n",
    "        ### Assistant\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa22758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### System Prompt\n",
      "        ### User Message\n",
      "              The assistant is not able to give a response at this time.\n",
      "        ### Assistant\n",
      "                ### Error\n",
      "                              The error was not able to be fixed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chain.run(text=\"\"\"### System Prompt\n",
    "        ### User Message\n",
    "              Tell me more about other code.\n",
    "        ### Assistant\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\n",
    "#     chain.run(\n",
    "#         text=\"\"\"[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
    "#         In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month\n",
    "#         [/INST]\"\"\"\n",
    "#     )\n",
    "# )\n",
    "# print(chain.run(text=\"[INST]\\nTell me more about other code.[\\INST]\\n\\n\"))\n",
    "\n",
    "# print(\n",
    "#     chain.run(\n",
    "#         text=\"[INST]\\nWhat can I see in Vienna? Propose a few locations. Names only, no details.[\\INST]\\n\\n\"\n",
    "#     )\n",
    "# )\n",
    "# print(chain.run(text=\"[INST]\\nTell me more about #2.[\\INST]\\n\\n\"))\n",
    "\n",
    "# print(\n",
    "#     chain.run(\n",
    "#         text=\"대한민국에 있는 유명한 산 이름을 10개 정도 나열해주세요.\"\n",
    "#     )\n",
    "# )\n",
    "# print(chain.run(text=\"Tell me more about #2.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
